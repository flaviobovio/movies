{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45451 entries, 0 to 45450\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0             45451 non-null  int64  \n",
      " 1   belongs_to_collection  4497 non-null   object \n",
      " 2   budget                 45451 non-null  float64\n",
      " 3   genres                 45451 non-null  object \n",
      " 4   id                     45451 non-null  int64  \n",
      " 5   original_language      45440 non-null  object \n",
      " 6   overview               44510 non-null  object \n",
      " 7   popularity             45451 non-null  float64\n",
      " 8   production_companies   45451 non-null  object \n",
      " 9   production_countries   45451 non-null  object \n",
      " 10  release_date           45451 non-null  object \n",
      " 11  revenue                45451 non-null  float64\n",
      " 12  runtime                45205 non-null  float64\n",
      " 13  spoken_languages       45451 non-null  object \n",
      " 14  status                 45371 non-null  object \n",
      " 15  tagline                20425 non-null  object \n",
      " 16  title                  45451 non-null  object \n",
      " 17  vote_average           45451 non-null  float64\n",
      " 18  vote_count             45451 non-null  float64\n",
      " 19  release_year           45451 non-null  int64  \n",
      " 20  return                 45451 non-null  float64\n",
      " 21  director               44728 non-null  object \n",
      "dtypes: float64(7), int64(3), object(12)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/movies_clean.csv')\n",
    "df.sample(5)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        toy story led by woody andys toys live happily...\n",
       "1        jumanji when siblings judy and peter discover ...\n",
       "2        grumpier old men a family wedding reignites th...\n",
       "3        waiting to exhale cheated on mistreated and st...\n",
       "4        father of the bride part ii just when george b...\n",
       "                               ...                        \n",
       "45446    robin hood yet another version of the classic ...\n",
       "45447    century of birthing an artist struggles to fin...\n",
       "45448    betrayal when one of her hits goes wrong a pro...\n",
       "45449    satan triumphant in a small town live two brot...\n",
       "45450    queerama 50 years after decriminalisation of h...\n",
       "Length: 45451, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase,remove punctuation\n",
    "    .'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "q = df['title'] + \" \" + df['overview']\n",
    "q  = q.apply(lambda x:clean_text(x))\n",
    "\n",
    "\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4590/4005089152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mq_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "#create the TF-IDF model\n",
    "# MAX_DF     = 0.95\n",
    "# MIN_DF     = 1#2\n",
    "# tfidf = TfidfVectorizer(max_df=MAX_DF, min_df=MIN_DF, stop_words='english')\n",
    "\n",
    "content = q.dropna()[:30000]\n",
    "count_vectorizer = CountVectorizer(token_pattern = r\"\\b\\w{5,}\\b\", stop_words='english')\n",
    "#tfidf_matrix = tfidf.fit_transform(content)\n",
    "\n",
    "\n",
    "#cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "count_matrix = count_vectorizer.fit_transform(content)\n",
    "\n",
    "\n",
    "#cosine_similarities = linear_kernel(count_matrix, count_matrix)\n",
    "\n",
    "# q = q.dropna()\n",
    "# tfidf_vectorizer.fit(q)\n",
    "# q_tfidf = tfidf_vectorizer.transform((q))\n",
    "\n",
    "\n",
    "#cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cosine_similarities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4590/3110592696.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosine_similarity_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcosine_similarity_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcosine_similarity_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cosine_similarities' is not defined"
     ]
    }
   ],
   "source": [
    "cosine_similarity_scores = list(enumerate(cosine_similarities[0]))\n",
    "cosine_similarity_scores = sorted(cosine_similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "cosine_similarity_scores\n",
    "\n",
    "for i in range(6):\n",
    "    idx = cosine_similarity_scores[i]\n",
    "    print (idx)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el lemmatizar de NLTK, y creamos el objeto\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ozzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ozzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Esto sirve para configurar NLTK. La primera vez puede tardar un poco\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "pp =nltk.word_tokenize(df.overview[0])\n",
    "print (len(pp))\n",
    "pp = [word for word in pp if word not in stopwords]\n",
    "print (len(pp))\n",
    "#df.overview.apply(lambda p : nltk.word_tokenize(p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la función que nos permite Stemmizar de nltk y definimos el stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "# # Recorremos todos los titulos y le vamos aplicando la Normalizacion y luega el Stemming a cada uno\n",
    "# for row in df:\n",
    "#     titular = row.overview\n",
    "#     # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "#     titular=re.sub(\"[^a-zA-Z]\",\" \",str(titular))\n",
    "#     # Pasamos todo a minúsculas\n",
    "#     titular=titular.lower()\n",
    "#     # Tokenizamos para separar las palabras del titular\n",
    "#     titular=nltk.word_tokenize(titular)\n",
    "#     # Eliminamos las palabras de menos de 3 letras\n",
    "#     titular = [palabra for palabra in titular if len(palabra)>3]\n",
    "#     # Sacamos las Stopwords\n",
    "#     titular = [palabra for palabra in titular if not palabra in stopwords]\n",
    "    \n",
    "#     ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "    \n",
    "#     # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "#     titular=[stemmer.stem(palabra) for palabra in titular]\n",
    "#     # Por ultimo volvemos a unir el titular\n",
    "#     titular=\" \".join(titular)\n",
    "    \n",
    "#     # Vamos armando una lista con todos los titulares\n",
    "#     titular_list.append(titular)\n",
    "#     #dataset[\"titular_normalizado\"] = titular_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content(row):\n",
    "\n",
    "    titular = row.overview\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    titular=re.sub(\"[^a-zA-Z]\",\" \",str(titular))\n",
    "    # Pasamos todo a minúsculas\n",
    "    titular=titular.lower()\n",
    "    # Tokenizamos para separar las palabras del titular\n",
    "    titular=nltk.word_tokenize(titular)\n",
    "    # Eliminamos las palabras de menos de 3 letras\n",
    "    titular = [palabra for palabra in titular if len(palabra)>3]\n",
    "    # Sacamos las Stopwords\n",
    "    titular = [palabra for palabra in titular if not palabra in stopwords]\n",
    "    \n",
    "    ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "    \n",
    "    # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "    titular=[stemmer.stem(palabra) for palabra in titular]\n",
    "    # Por ultimo volvemos a unir el titular\n",
    "    titular= row.title.lower() + \" \" + \" \".join(titular)\n",
    "\n",
    "    return titular\n",
    "\n",
    "df['content'] = df.apply(content, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        45451\n",
       "unique       42195\n",
       "top       Blackout\n",
       "freq            13\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#create the TF-IDF model\n",
    "MAX_DF     = 0.95\n",
    "MIN_DF     = 1\n",
    "tfidf = TfidfVectorizer(#max_df=MAX_DF, min_df=MIN_DF, \\\n",
    "                        #max_features=10000,\\\n",
    "                        ngram_range=(1,2),\n",
    "                        token_pattern = r\"\\b\\w{5,}\\b\")\n",
    "\n",
    "content = df['content'].dropna()[:20000]\n",
    "tfidf_matrix = tfidf.fit_transform(content)\n",
    "\n",
    "#tfidf_matrix.shape()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from datetime import datetime\n",
    "tfidf_matrix = sparse.load_npz(\"data/tfidf_matrix.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "\n",
    "np_arr = np.array([1.3, 4.22, -5], dtype=np.float32)\n",
    "pa_table = pa.table({\"data\": cosine_similarities})\n",
    "pa.parquet.write_table(pa_table, \"data/cosine_similarities.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "matrix = cosine_similarities\n",
    "arrays = [\n",
    "    pa.array(col)  # Create one arrow array per column\n",
    "    for col in matrix\n",
    "]\n",
    "\n",
    "table = pa.Table.from_arrays(\n",
    "    arrays,\n",
    "    names=[str(i) for i in range(len(arrays))] # give names to each columns\n",
    ")\n",
    "# Save it:\n",
    "pq.write_table(table, 'data/cosine_similarities.pq')\n",
    "\n",
    "# # Read it back as numpy:\n",
    "# table_from_parquet = pq.read_table('table.pq')\n",
    "# matrix_from_parquet = table_from_parquet.to_pandas().T.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(46, 1.0000000000000004), (1672, 0.11158651823928598), (21, 0.11087828258439557), (476, 0.1063867805038503), (14129, 0.0998881423463085)]\n",
      "Se7en 1.0000000000000004\n",
      "Fallen 0.11158651823928598\n",
      "Copycat 0.11087828258439557\n",
      "Kalifornia 0.1063867805038503\n",
      "The Cell 2 0.0998881423463085\n",
      "Murder by Numbers 0.09956969395686152\n",
      "Tiempo :  5.87\n"
     ]
    }
   ],
   "source": [
    "t = datetime.now()\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "# Read it back as numpy:\n",
    "table_from_parquet = pq.read_table('data/cosine_similarities.pq')\n",
    "cosine_similarities = table_from_parquet.to_pandas().T.to_numpy()\n",
    "\n",
    "#cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "#print (type (cosine_similarities))\n",
    "print ('Tiempo : ', round((datetime.now()-t).total_seconds(), 2) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0000000000000002), (3006, 0.1846449436245275), (15406, 0.17917450140839977), (1938, 0.10480073050179864), (1039, 0.0860263851872323)]\n",
      "Toy Story 1.0000000000000002\n",
      "Toy Story 2 0.1846449436245275\n",
      "Toy Story 3 0.17917450140839977\n",
      "Condorman 0.10480073050179864\n",
      "The Sunchaser 0.0860263851872323\n",
      "Bound for Glory 0.07745464529937668\n",
      "Tiempo :  0.02\n"
     ]
    }
   ],
   "source": [
    "t = datetime.now()\n",
    "cosine_similarity_scores = list(enumerate(cosine_similarities[0]))\n",
    "cosine_similarity_scores = sorted(cosine_similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "print (cosine_similarity_scores[:5])\n",
    "\n",
    "for i in cosine_similarity_scores[:6]:\n",
    "    print (df.loc[i[0]].title, i[1])\n",
    "\n",
    "print ('Tiempo : ', round((datetime.now()-t).total_seconds(), 2) )\n",
    "# print (len(df.loc[123].overview))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
