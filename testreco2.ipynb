{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45429 entries, 0 to 45428\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   belongs_to_collection  4495 non-null   object \n",
      " 1   budget                 45429 non-null  float64\n",
      " 2   genres                 45429 non-null  object \n",
      " 3   id                     45429 non-null  int64  \n",
      " 4   original_language      45418 non-null  object \n",
      " 5   overview               45429 non-null  object \n",
      " 6   popularity             45429 non-null  object \n",
      " 7   production_companies   45429 non-null  object \n",
      " 8   production_countries   45429 non-null  object \n",
      " 9   release_date           45429 non-null  object \n",
      " 10  revenue                45429 non-null  float64\n",
      " 11  runtime                45183 non-null  float64\n",
      " 12  spoken_languages       45429 non-null  object \n",
      " 13  status                 45349 non-null  object \n",
      " 14  tagline                20416 non-null  object \n",
      " 15  title                  45429 non-null  object \n",
      " 16  vote_average           45429 non-null  float64\n",
      " 17  vote_count             45429 non-null  float64\n",
      " 18  release_year           45429 non-null  int64  \n",
      " 19  return                 45429 non-null  float64\n",
      " 20  director               45429 non-null  object \n",
      "dtypes: float64(6), int64(2), object(13)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data/movies.pq')\n",
    "df.sample(5)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 45429 entries, 0 to 45428\n",
      "Series name: production_countries\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "45429 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 709.8+ KB\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mproduction_countries\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39minfo()\n\u001b[0;32m----> 2\u001b[0m df[\u001b[39m'\u001b[39;49m\u001b[39mUnited States of America\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39min\u001b[39;49;00m df[\u001b[39m'\u001b[39;49m\u001b[39mproduction_countries\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3631\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3633\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "\n",
    "df['production_countries'].info()\n",
    "df['United States of America' in df['production_countries']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase,remove punctuation\n",
    "    .'''\n",
    "    text = str(text).lower().replace('collection','')\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "df['belongs_to_collection'].fillna('', inplace=True)\n",
    "q = df['title'] + \" \" + df['overview'] + \" \" + df['genres'] + ' ' + df['belongs_to_collection']\n",
    "\n",
    "#q = df['title'] +  \" \" + df['genres'] + ' ' + df['belongs_to_collection']\n",
    "q  = q.apply(lambda x:clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        toy story led by woody andys toys live happily...\n",
       "1        jumanji when siblings judy and peter discover ...\n",
       "2        grumpier old men a family wedding reignites th...\n",
       "3        waiting to exhale cheated on mistreated and st...\n",
       "4        father of the bride part ii just when george b...\n",
       "                               ...                        \n",
       "45424    robin hood yet another version of the classic ...\n",
       "45425    century of birthing an artist struggles to fin...\n",
       "45426    betrayal when one of her hits goes wrong a pro...\n",
       "45427    satan triumphant in a small town live two brot...\n",
       "45428    queerama 50 years after decriminalisation of h...\n",
       "Length: 45429, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ten canoes a story within a story within a story in australias northern territory an aboriginal narrator tells a story about his ancestors on a goose hunt a youngster on the hunt is being tempted to adultery with his elder brothers wife so an elder tells him a story from the mythical past about how evil can slip in and cause havoc unless prevented by virtue according to customary tribal law adventure comedy drama '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[11882]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the TF-IDF model\n",
    "# MAX_DF     = 0.95\n",
    "# MIN_DF     = 1#2\n",
    "tfidf = TfidfVectorizer(token_pattern = r\"\\b\\w{3,}\\b\", stop_words='english',  ngram_range=(1,2))\n",
    "\n",
    "content = q.dropna()[:15000]\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(content)\n",
    "\n",
    "\n",
    "cosine_similarities = None\n",
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "\n",
    "content = q.dropna()[:15000]\n",
    "count_vectorizer = CountVectorizer(token_pattern = r\"\\b\\w{3,}\\b\", stop_words='english',  ngram_range=(1,2))\n",
    "q = q.dropna()\n",
    "count_matrix = count_vectorizer.fit_transform(content)\n",
    "cosine_similarities = None\n",
    "cosine_similarities = linear_kernel(count_matrix, count_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Story\n",
      "(3005, 0.34359414470660926) Toy Story 2\n",
      "(1078, 0.06640693209089227) Rebel Without a Cause\n",
      "(485, 0.06563615813973897) Malice\n",
      "(10968, 0.05415495403008784) The Wild\n",
      "(11433, 0.0537268354115776) For Your Consideration\n"
     ]
    }
   ],
   "source": [
    "id_movie = 0\n",
    "cosine_similarity_scores = list(enumerate(cosine_similarities[id_movie]))\n",
    "cosine_similarity_scores = sorted(cosine_similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "cosine_similarity_scores\n",
    "\n",
    "print (df.iloc[id_movie].title)\n",
    "for i in range(1, 6):\n",
    "    idx = cosine_similarity_scores[i]\n",
    "    print (idx, df.iloc[idx[0]].title)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el lemmatizar de NLTK, y creamos el objeto\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ozzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ozzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Esto sirve para configurar NLTK. La primera vez puede tardar un poco\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "pp =nltk.word_tokenize(df.overview[0])\n",
    "print (len(pp))\n",
    "pp = [word for word in pp if word not in stopwords]\n",
    "print (len(pp))\n",
    "#df.overview.apply(lambda p : nltk.word_tokenize(p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la función que nos permite Stemmizar de nltk y definimos el stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "# # Recorremos todos los titulos y le vamos aplicando la Normalizacion y luega el Stemming a cada uno\n",
    "# for row in df:\n",
    "#     titular = row.overview\n",
    "#     # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "#     titular=re.sub(\"[^a-zA-Z]\",\" \",str(titular))\n",
    "#     # Pasamos todo a minúsculas\n",
    "#     titular=titular.lower()\n",
    "#     # Tokenizamos para separar las palabras del titular\n",
    "#     titular=nltk.word_tokenize(titular)\n",
    "#     # Eliminamos las palabras de menos de 3 letras\n",
    "#     titular = [palabra for palabra in titular if len(palabra)>3]\n",
    "#     # Sacamos las Stopwords\n",
    "#     titular = [palabra for palabra in titular if not palabra in stopwords]\n",
    "    \n",
    "#     ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "    \n",
    "#     # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "#     titular=[stemmer.stem(palabra) for palabra in titular]\n",
    "#     # Por ultimo volvemos a unir el titular\n",
    "#     titular=\" \".join(titular)\n",
    "    \n",
    "#     # Vamos armando una lista con todos los titulares\n",
    "#     titular_list.append(titular)\n",
    "#     #dataset[\"titular_normalizado\"] = titular_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content(row):\n",
    "\n",
    "    titular = row.overview\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    titular=re.sub(\"[^a-zA-Z]\",\" \",str(titular))\n",
    "    # Pasamos todo a minúsculas\n",
    "    titular=titular.lower()\n",
    "    # Tokenizamos para separar las palabras del titular\n",
    "    titular=nltk.word_tokenize(titular)\n",
    "    # Eliminamos las palabras de menos de 3 letras\n",
    "    titular = [palabra for palabra in titular if len(palabra)>3]\n",
    "    # Sacamos las Stopwords\n",
    "    titular = [palabra for palabra in titular if not palabra in stopwords]\n",
    "    \n",
    "    ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "    \n",
    "    # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "    titular=[stemmer.stem(palabra) for palabra in titular]\n",
    "    # Por ultimo volvemos a unir el titular\n",
    "    titular= row.title.lower() + \" \" + \" \".join(titular)\n",
    "\n",
    "    return titular\n",
    "\n",
    "df['content'] = df.apply(content, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        45451\n",
       "unique       42195\n",
       "top       Blackout\n",
       "freq            13\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#create the TF-IDF model\n",
    "MAX_DF     = 0.95\n",
    "MIN_DF     = 1\n",
    "tfidf = TfidfVectorizer(#max_df=MAX_DF, min_df=MIN_DF, \\\n",
    "                        #max_features=10000,\\\n",
    "                        ngram_range=(1,2),\n",
    "                        token_pattern = r\"\\b\\w{5,}\\b\")\n",
    "\n",
    "content = df['content'].dropna()[:20000]\n",
    "tfidf_matrix = tfidf.fit_transform(content)\n",
    "\n",
    "#tfidf_matrix.shape()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "\n",
    "np_arr = np.array([1.3, 4.22, -5], dtype=np.float32)\n",
    "pa_table = pa.table({\"data\": cosine_similarities})\n",
    "pa.parquet.write_table(pa_table, \"data/cosine_similarities.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "matrix = cosine_similarities\n",
    "arrays = [\n",
    "    pa.array(col)  # Create one arrow array per column\n",
    "    for col in matrix\n",
    "]\n",
    "\n",
    "table = pa.Table.from_arrays(\n",
    "    arrays,\n",
    "    names=[str(i) for i in range(len(arrays))] # give names to each columns\n",
    ")\n",
    "# Save it:\n",
    "pq.write_table(table, 'data/cosine_similarities.pq')\n",
    "\n",
    "# # Read it back as numpy:\n",
    "# table_from_parquet = pq.read_table('table.pq')\n",
    "# matrix_from_parquet = table_from_parquet.to_pandas().T.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(46, 1.0000000000000004), (1672, 0.11158651823928598), (21, 0.11087828258439557), (476, 0.1063867805038503), (14129, 0.0998881423463085)]\n",
      "Se7en 1.0000000000000004\n",
      "Fallen 0.11158651823928598\n",
      "Copycat 0.11087828258439557\n",
      "Kalifornia 0.1063867805038503\n",
      "The Cell 2 0.0998881423463085\n",
      "Murder by Numbers 0.09956969395686152\n",
      "Tiempo :  5.87\n"
     ]
    }
   ],
   "source": [
    "t = datetime.now()\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "# Read it back as numpy:\n",
    "table_from_parquet = pq.read_table('data/cosine_similarities.pq')\n",
    "cosine_similarities = table_from_parquet.to_pandas().T.to_numpy()\n",
    "\n",
    "#cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "#print (type (cosine_similarities))\n",
    "print ('Tiempo : ', round((datetime.now()-t).total_seconds(), 2) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0000000000000002), (3006, 0.1846449436245275), (15406, 0.17917450140839977), (1938, 0.10480073050179864), (1039, 0.0860263851872323)]\n",
      "Toy Story 1.0000000000000002\n",
      "Toy Story 2 0.1846449436245275\n",
      "Toy Story 3 0.17917450140839977\n",
      "Condorman 0.10480073050179864\n",
      "The Sunchaser 0.0860263851872323\n",
      "Bound for Glory 0.07745464529937668\n",
      "Tiempo :  0.02\n"
     ]
    }
   ],
   "source": [
    "t = datetime.now()\n",
    "cosine_similarity_scores = list(enumerate(cosine_similarities[0]))\n",
    "cosine_similarity_scores = sorted(cosine_similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "print (cosine_similarity_scores[:5])\n",
    "\n",
    "for i in cosine_similarity_scores[:6]:\n",
    "    print (df.loc[i[0]].title, i[1])\n",
    "\n",
    "print ('Tiempo : ', round((datetime.now()-t).total_seconds(), 2) )\n",
    "# print (len(df.loc[123].overview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12359/171254172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'The Terminator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0mnew_self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_axis_number\u001b[0;34m(cls, axis)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_TO_AXIS_NUMBER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No axis named {axis} for object type {cls.__name__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Series'"
     ]
    }
   ],
   "source": [
    "df.title.loc(df.title=='The Terminator')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
